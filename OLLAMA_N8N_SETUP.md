# Ollama + N8N via NGINX Proxy

Ce document explique comment utiliser Ollama depuis N8N via le proxy NGINX pour avoir une observabilit√© compl√®te des prompts et r√©ponses LLM.

## üéØ Avantages

- ‚úÖ **Tous les prompts/r√©ponses logg√©s** dans Grafana/Loki
- ‚úÖ **Temps de g√©n√©ration track√©s** pour chaque requ√™te
- ‚úÖ **URL unifi√©e** : `http://nginx:80/api/ollama/`
- ‚úÖ **Pas besoin de g√©rer** `host.docker.internal` dans N8N

---

## üìã Pr√©requis

### Configuration d'Ollama

Ollama doit √©couter sur toutes les interfaces pour √™tre accessible depuis Docker :

```bash
# V√©rifier la configuration actuelle
lsof -iTCP:11434 | grep LISTEN

# Devrait montrer : *:11434 (ou 0.0.0.0:11434)
# Si √ßa montre 127.0.0.1:11434, configurer OLLAMA_HOST
```

### Rendre la configuration permanente

**Sur macOS, ajouter dans `~/.zshrc` ou `~/.bash_profile` :**

```bash
export OLLAMA_HOST=0.0.0.0:11434
```

Puis :
```bash
source ~/.zshrc
killall ollama
ollama serve &
```

**Sur Linux avec systemd :**

```bash
sudo systemctl edit ollama

# Ajouter :
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"

# Sauvegarder et red√©marrer
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

---

## üîß Configuration dans N8N

### URL √† utiliser dans N8N

**Depuis n'importe quel node N8N :**
```
http://nginx:80/api/ollama/
```

---

## üí° Exemples d'utilisation

### 1. Node HTTP Request - Lister les mod√®les

**Configuration :**
- **Method :** GET
- **URL :** `http://nginx:80/api/ollama/api/tags`

**R√©ponse :**
```json
{
  "models": [
    {
      "name": "llama3.1:8b",
      "size": 4920753328,
      "details": {
        "parameter_size": "8.0B"
      }
    }
  ]
}
```

---

### 2. Node HTTP Request - G√©n√©rer du texte (sans streaming)

**Configuration :**
- **Method :** POST
- **URL :** `http://nginx:80/api/ollama/api/generate`
- **Body Type :** JSON
- **Body :**

```json
{
  "model": "llama3.1:8b",
  "prompt": "{{ $json.question }}",
  "stream": false,
  "options": {
    "temperature": 0.7,
    "top_p": 0.9
  }
}
```

**Utilisation des variables N8N :**
```json
{
  "model": "{{ $json.model || 'llama3.1:8b' }}",
  "prompt": "{{ $json.prompt }}",
  "stream": false
}
```

**R√©ponse :**
```json
{
  "model": "llama3.1:8b",
  "response": "Voici la r√©ponse g√©n√©r√©e...",
  "done": true,
  "total_duration": 5423789000,
  "load_duration": 1234567,
  "prompt_eval_count": 25,
  "eval_count": 150
}
```

---

### 3. Node HTTP Request - Chat avec contexte

**Configuration :**
- **Method :** POST
- **URL :** `http://nginx:80/api/ollama/api/chat`
- **Body :**

```json
{
  "model": "llama3.1:8b",
  "messages": [
    {
      "role": "system",
      "content": "Tu es un assistant utile et pr√©cis."
    },
    {
      "role": "user",
      "content": "{{ $json.question }}"
    }
  ],
  "stream": false
}
```

**Avec historique de conversation :**
```json
{
  "model": "llama3.1:8b",
  "messages": {{ $json.conversation_history }},
  "stream": false
}
```

**R√©ponse :**
```json
{
  "message": {
    "role": "assistant",
    "content": "Je suis l√† pour vous aider..."
  },
  "done": true
}
```

---

### 4. Node HTTP Request - Embeddings

**Configuration :**
- **Method :** POST
- **URL :** `http://nginx:80/api/ollama/api/embeddings`
- **Body :**

```json
{
  "model": "nomic-embed-text:latest",
  "prompt": "{{ $json.text }}"
}
```

**R√©ponse :**
```json
{
  "embedding": [0.123, 0.456, 0.789, ...]
}
```

---

## üîÑ Workflow N8N complet : Agent IA + Logging

Voici un workflow type pour utiliser Ollama avec logging automatique dans Braintrust :

### Node 1 : Webhook Trigger
- Re√ßoit `{ "question": "..." }`

### Node 2 : HTTP Request - Ollama
- **URL :** `http://nginx:80/api/ollama/api/generate`
- **Body :**
```json
{
  "model": "llama3.1:8b",
  "prompt": "{{ $json.question }}",
  "stream": false
}
```

### Node 3 : Code Node - Format Response
```javascript
return {
  json: {
    question: $input.first().json.question,
    answer: $json.response,
    model: $json.model,
    generation_time: ($json.total_duration / 1000000000).toFixed(2) + 's',
    tokens: $json.eval_count
  }
};
```

### Node 4 : HTTP Request - Log to Braintrust
- **URL :** `http://nginx:80/api/braintrust/project_logs/YOUR_PROJECT_ID/insert`
- **Body :**
```json
{
  "events": [{
    "input": "{{ $('Node 3').item.json.question }}",
    "output": "{{ $('Node 3').item.json.answer }}",
    "metadata": {
      "model": "{{ $('Node 3').item.json.model }}",
      "generation_time": "{{ $('Node 3').item.json.generation_time }}",
      "tokens": "{{ $('Node 3').item.json.tokens }}"
    }
  }]
}
```

### Node 5 : Respond to Webhook
- Retourne la r√©ponse √† l'utilisateur

---

## üìä Observabilit√© dans Grafana

### Voir tous les appels Ollama

**Dans Grafana Explore (http://localhost:3000) :**

```logql
{job="nginx", service="ollama"} | json
```

### Voir les prompts envoy√©s

```logql
{job="nginx", service="ollama", method="POST"}
| json
| line_format "Prompt: {{.request_body}}"
```

### Analyser les temps de g√©n√©ration

**Temps moyen sur 5 minutes :**
```logql
avg(avg_over_time({job="nginx", service="ollama"} | json | unwrap request_time [5m]))
```

**Requ√™tes lentes (> 10 secondes) :**
```logql
{job="nginx", service="ollama"} | json | request_time > 10
```

### Compter les requ√™tes par mod√®le

```logql
{job="nginx", service="ollama"}
| json
| request_body =~ "model"
| line_format "{{.request_body}}"
```

### Dashboard Grafana sugg√©r√©

**Panel 1 : Nombre de requ√™tes (5 min)**
```logql
sum(count_over_time({job="nginx", service="ollama"}[5m]))
```

**Panel 2 : Temps de g√©n√©ration moyen**
```logql
avg(avg_over_time({job="nginx", service="ollama"} | json | unwrap request_time [5m]))
```

**Panel 3 : Distribution des temps de r√©ponse**
```logql
histogram_quantile(0.95,
  sum(rate({job="nginx", service="ollama"} | json | unwrap request_time [5m])) by (le)
)
```

**Panel 4 : Taux d'erreur**
```logql
sum(rate({job="nginx", service="ollama"} | json | status >= 400 [5m]))
/
sum(rate({job="nginx", service="ollama"}[5m]))
```

---

## üîç Exemple de logs captur√©s

```json
{
  "time": "2026-01-06T17:58:28+00:00",
  "remote_addr": "172.18.0.6",
  "request_id": "8cbd07b76e4857ad7cb0dffe9acf0d86",
  "method": "POST",
  "host": "nginx",
  "request_uri": "/api/ollama/api/generate",
  "uri": "/api/generate",
  "status": 200,
  "request_time": 7.627,
  "upstream_status": "200",
  "request_body": "{\"model\":\"llama3.1:8b\",\"prompt\":\"Explique Docker\",\"stream\":false}",
  "content_type": "application/json",
  "content_length": "95"
}
```

**Informations logg√©es :**
- ‚úÖ Prompt complet
- ‚úÖ Mod√®le utilis√©
- ‚úÖ Temps de g√©n√©ration (7.627 secondes)
- ‚úÖ Statut de la r√©ponse
- ‚úÖ ID de requ√™te unique

---

## üöÄ Tests rapides

### Depuis l'h√¥te (votre machine)

```bash
# Lister les mod√®les
curl http://localhost:8080/api/ollama/api/tags

# G√©n√©rer du texte
curl http://localhost:8080/api/ollama/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "Bonjour",
  "stream": false
}'

# Chat
curl http://localhost:8080/api/ollama/api/chat -d '{
  "model": "llama3.1:8b",
  "messages": [{"role": "user", "content": "Hello"}],
  "stream": false
}'
```

### Depuis N8N (conteneur Docker)

Dans un node HTTP Request :
- **URL :** `http://nginx:80/api/ollama/api/tags`
- **Method :** GET

---

## ‚öôÔ∏è Options de g√©n√©ration avanc√©es

### Temp√©rature et cr√©ativit√©

```json
{
  "model": "llama3.1:8b",
  "prompt": "√âcris une histoire",
  "stream": false,
  "options": {
    "temperature": 0.9,     // Plus √©lev√© = plus cr√©atif (0.0 - 2.0)
    "top_p": 0.95,          // Nucleus sampling (0.0 - 1.0)
    "top_k": 40,            // Top-K sampling
    "repeat_penalty": 1.1,  // P√©nalit√© pour r√©p√©tition
    "num_predict": 500      // Nombre max de tokens g√©n√©r√©s
  }
}
```

### Contexte et prompt system

```json
{
  "model": "llama3.1:8b",
  "prompt": "Question utilisateur",
  "system": "Tu es un expert en Docker et Kubernetes",
  "stream": false
}
```

---

## üîí S√©curit√©

### Attention avec OLLAMA_HOST=0.0.0.0

Configurer Ollama sur `0.0.0.0:11434` signifie qu'il accepte les connexions depuis n'importe quelle interface r√©seau.

**C'est s√ªr si :**
- ‚úÖ Vous √™tes derri√®re un routeur/firewall
- ‚úÖ Le firewall de votre OS est activ√©
- ‚úÖ Vous ne partagez pas votre connexion

**Pour plus de s√©curit√© :**
- Configurer des r√®gles de firewall pour autoriser uniquement Docker
- Utiliser un VPN ou tunnel pour les connexions externes
- Ne pas exposer le port 11434 sur Internet

---

## üêõ Troubleshooting

### Erreur : Connection refused

```bash
# V√©rifier qu'Ollama √©coute sur toutes les interfaces
lsof -iTCP:11434 | grep LISTEN
# Doit montrer : *:11434 (pas 127.0.0.1:11434)

# Si ce n'est pas le cas, reconfigurer
export OLLAMA_HOST=0.0.0.0:11434
killall ollama
ollama serve &
```

### Erreur 403 depuis N8N

```bash
# Tester depuis N8N
docker exec n8n wget -qO- http://nginx:80/health

# Si √ßa ne fonctionne pas, v√©rifier le r√©seau Docker
docker network inspect self-hosted-ai-starter-kit_demo
```

### Logs ne s'affichent pas dans Grafana

```bash
# V√©rifier que Promtail collecte les logs Ollama
docker logs promtail | grep ollama

# V√©rifier les logs NGINX
docker exec nginx ls -lh /var/log/nginx/ | grep ollama

# Tester une requ√™te
curl http://localhost:8080/api/ollama/api/tags

# V√©rifier le log
docker exec nginx tail -1 /var/log/nginx/ollama-access.log
```

### Ollama ne r√©pond pas / timeout

Les timeouts sont configur√©s √† 300 secondes (5 minutes) dans NGINX. Pour les ajuster :

Modifier `nginx/proxy.conf.template` :
```nginx
location /api/ollama/ {
    proxy_connect_timeout 600s;  # 10 minutes
    proxy_send_timeout 600s;
    proxy_read_timeout 600s;
    # ...
}
```

Puis red√©marrer :
```bash
docker restart nginx
```

---

## üìñ Documentation officielle

- **Ollama API :** https://github.com/ollama/ollama/blob/main/docs/api.md
- **N8N HTTP Request :** https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.httprequest/
- **Grafana LogQL :** https://grafana.com/docs/loki/latest/logql/

---

**Derni√®re mise √† jour :** 6 janvier 2026
