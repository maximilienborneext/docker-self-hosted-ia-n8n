# Configuration Ollama pour Docker

Ce guide explique comment configurer et d√©marrer Ollama correctement pour qu'il soit accessible depuis les conteneurs Docker (N8N, NGINX proxy, etc.).

## üéØ Objectif

Par d√©faut, Ollama √©coute uniquement sur `127.0.0.1:11434` (localhost), ce qui emp√™che les conteneurs Docker d'y acc√©der. Nous devons le configurer pour √©couter sur toutes les interfaces (`0.0.0.0:11434`).

---

## ‚ö° D√©marrage rapide

### 1. Arr√™ter Ollama

```bash
killall ollama
```

### 2. D√©marrer Ollama avec la bonne configuration

```bash
OLLAMA_HOST=0.0.0.0:11434 ollama serve
```

### 3. V√©rifier la configuration

```bash
lsof -iTCP:11434 | grep LISTEN
```

**R√©sultat attendu :**
```
ollama  12345 francetv  3u  IPv6 0x...  0t0  TCP *:11434 (LISTEN)
                                              ^^^
                                              ‚úÖ √âcoute sur toutes les interfaces
```

**‚ùå Mauvaise configuration :**
```
ollama  12345 francetv  3u  IPv4 0x...  0t0  TCP localhost:11434 (LISTEN)
                                              ^^^^^^^^^^^^
                                              ‚ùå √âcoute uniquement sur localhost
```

---

## üîß Configuration permanente

Pour ne pas avoir √† d√©finir `OLLAMA_HOST` √† chaque d√©marrage, ajoutez-le √† votre fichier de configuration shell.

### Sur macOS/Linux avec Zsh (par d√©faut sur macOS)

**1. √âditer `~/.zshrc` :**

```bash
nano ~/.zshrc
# ou
vim ~/.zshrc
# ou
code ~/.zshrc
```

**2. Ajouter √† la fin du fichier :**

```bash
# Configuration Ollama pour Docker
export OLLAMA_HOST=0.0.0.0:11434
```

**3. Recharger la configuration :**

```bash
source ~/.zshrc
```

**4. Red√©marrer Ollama :**

```bash
killall ollama
ollama serve &
```

### Sur macOS/Linux avec Bash

**1. √âditer `~/.bash_profile` ou `~/.bashrc` :**

```bash
nano ~/.bash_profile
```

**2. Ajouter √† la fin du fichier :**

```bash
# Configuration Ollama pour Docker
export OLLAMA_HOST=0.0.0.0:11434
```

**3. Recharger la configuration :**

```bash
source ~/.bash_profile
```

**4. Red√©marrer Ollama :**

```bash
killall ollama
ollama serve &
```

### Sur Linux avec systemd (installation syst√®me)

**1. Cr√©er un fichier de configuration systemd :**

```bash
sudo systemctl edit ollama
```

**2. Ajouter la configuration :**

```ini
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"
```

**3. Sauvegarder et quitter (Ctrl+X, puis Y, puis Entr√©e)**

**4. Recharger et red√©marrer :**

```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

**5. V√©rifier le statut :**

```bash
sudo systemctl status ollama
```

---

## üöÄ D√©marrage automatique

### macOS - D√©marrage automatique au login

**Option 1 : Avec launchd (recommand√©)**

**1. Cr√©er le fichier plist :**

```bash
nano ~/Library/LaunchAgents/com.ollama.serve.plist
```

**2. Copier cette configuration :**

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.ollama.serve</string>
    <key>ProgramArguments</key>
    <array>
        <string>/usr/local/bin/ollama</string>
        <string>serve</string>
    </array>
    <key>EnvironmentVariables</key>
    <dict>
        <key>OLLAMA_HOST</key>
        <string>0.0.0.0:11434</string>
    </dict>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
    <key>StandardOutPath</key>
    <string>/tmp/ollama.log</string>
    <key>StandardErrorPath</key>
    <string>/tmp/ollama.err</string>
</dict>
</plist>
```

**3. Charger le service :**

```bash
launchctl load ~/Library/LaunchAgents/com.ollama.serve.plist
```

**4. V√©rifier qu'il fonctionne :**

```bash
lsof -iTCP:11434 | grep LISTEN
```

**Commandes utiles :**

```bash
# Arr√™ter le service
launchctl unload ~/Library/LaunchAgents/com.ollama.serve.plist

# Red√©marrer le service
launchctl unload ~/Library/LaunchAgents/com.ollama.serve.plist
launchctl load ~/Library/LaunchAgents/com.ollama.serve.plist

# Voir les logs
tail -f /tmp/ollama.log
```

**Option 2 : Ajouter dans le fichier de d√©marrage du shell**

Dans `~/.zshrc` (ou `~/.bash_profile`) :

```bash
# D√©marrer Ollama automatiquement s'il n'est pas lanc√©
if ! pgrep -x "ollama" > /dev/null; then
    export OLLAMA_HOST=0.0.0.0:11434
    ollama serve > /tmp/ollama.log 2>&1 &
fi
```

---

## ‚úÖ V√©rification

### 1. V√©rifier qu'Ollama √©coute sur la bonne interface

```bash
lsof -iTCP:11434 | grep LISTEN
```

**R√©sultat attendu :**
```
ollama  12345 francetv  3u  IPv6 0x...  0t0  TCP *:11434 (LISTEN)
```

### 2. Tester depuis l'h√¥te

```bash
curl http://localhost:11434/api/tags
```

**R√©sultat attendu :** Liste des mod√®les Ollama en JSON

### 3. Tester depuis Docker (N8N)

```bash
docker exec n8n wget -qO- http://nginx:80/api/ollama/api/tags
```

**R√©sultat attendu :** Liste des mod√®les Ollama en JSON

### 4. Tester via le proxy NGINX

```bash
curl http://localhost:8080/api/ollama/api/tags
```

**R√©sultat attendu :** Liste des mod√®les Ollama en JSON

---

## üêõ Troubleshooting

### Probl√®me : Ollama ne d√©marre pas

**1. V√©rifier qu'aucun processus n'utilise le port 11434 :**

```bash
lsof -iTCP:11434
```

**2. Si un processus est en cours, le tuer :**

```bash
killall ollama
# ou
kill -9 <PID>
```

**3. Red√©marrer Ollama :**

```bash
OLLAMA_HOST=0.0.0.0:11434 ollama serve
```

### Probl√®me : Variable OLLAMA_HOST non prise en compte

**1. V√©rifier que la variable est d√©finie :**

```bash
echo $OLLAMA_HOST
# Devrait afficher : 0.0.0.0:11434
```

**2. Si vide, l'exporter manuellement :**

```bash
export OLLAMA_HOST=0.0.0.0:11434
```

**3. Red√©marrer Ollama :**

```bash
killall ollama
ollama serve &
```

### Probl√®me : "Connection refused" depuis Docker

**1. V√©rifier qu'Ollama √©coute sur `*:11434` et pas `127.0.0.1:11434` :**

```bash
lsof -iTCP:11434 | grep LISTEN
```

**2. V√©rifier que `host.docker.internal` est accessible :**

```bash
docker exec nginx ping -c 1 host.docker.internal
```

**3. Tester l'acc√®s direct :**

```bash
docker exec n8n wget -qO- http://host.docker.internal:11434/api/tags
```

### Probl√®me : Ollama red√©marre automatiquement

Si Ollama red√©marre automatiquement apr√®s un `killall`, c'est probablement un service syst√®me :

**Sur macOS avec launchd :**

```bash
# Lister les services Ollama
launchctl list | grep -i ollama

# D√©sactiver le service
launchctl unload ~/Library/LaunchAgents/com.ollama.serve.plist
```

**Sur Linux avec systemd :**

```bash
# Arr√™ter le service
sudo systemctl stop ollama

# D√©sactiver le d√©marrage automatique
sudo systemctl disable ollama
```

---

## üîí S√©curit√©

### ‚ö†Ô∏è Important

Configurer Ollama sur `0.0.0.0:11434` signifie qu'il accepte les connexions depuis **n'importe quelle interface r√©seau**.

### ‚úÖ C'est s√ªr si :

- Vous √™tes derri√®re un routeur avec NAT
- Le firewall de votre OS est activ√©
- Vous ne forwarding pas le port 11434 sur votre routeur
- Vous √™tes sur un r√©seau priv√©/domestique

### üõ°Ô∏è Protections recommand√©es

**1. Activer le firewall macOS :**

```bash
# Activer le firewall
sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setglobalstate on

# Autoriser Ollama
sudo /usr/libexec/ApplicationFirewall/socketfilterfw --add /usr/local/bin/ollama
```

**2. V√©rifier qu'aucune r√®gle de port forwarding n'existe :**

- Acc√©der √† l'interface de votre routeur
- V√©rifier qu'il n'y a pas de redirection du port 11434

**3. Limiter l'acc√®s au r√©seau local uniquement (optionnel) :**

Si vous voulez plus de s√©curit√©, vous pouvez utiliser un firewall pour limiter l'acc√®s :

```bash
# macOS - Bloquer l'acc√®s externe au port 11434
# (N√©cessite PF - Packet Filter)
```

---

## üìä R√©sum√© des commandes

### D√©marrage manuel

```bash
# Arr√™ter Ollama
killall ollama

# D√©marrer avec la bonne config
OLLAMA_HOST=0.0.0.0:11434 ollama serve

# En arri√®re-plan
OLLAMA_HOST=0.0.0.0:11434 ollama serve &
```

### Configuration permanente

```bash
# Ajouter √† ~/.zshrc ou ~/.bash_profile
echo 'export OLLAMA_HOST=0.0.0.0:11434' >> ~/.zshrc

# Recharger
source ~/.zshrc

# Red√©marrer Ollama
killall ollama
ollama serve &
```

### V√©rification

```bash
# V√©rifier le port
lsof -iTCP:11434 | grep LISTEN

# Tester l'API
curl http://localhost:11434/api/tags

# Tester depuis Docker
docker exec n8n wget -qO- http://nginx:80/api/ollama/api/tags
```

---

## üìö Prochaines √©tapes

Maintenant qu'Ollama est correctement configur√© :

1. **Consulter [OLLAMA_N8N_SETUP.md](./OLLAMA_N8N_SETUP.md)** pour utiliser Ollama dans N8N
2. **Consulter [NGINX_PROXY_SETUP.md](./NGINX_PROXY_SETUP.md)** pour comprendre le proxy NGINX
3. **Cr√©er vos premiers workflows N8N** avec Ollama

---

## üîó Liens utiles

- **Documentation officielle Ollama :** https://github.com/ollama/ollama
- **API Ollama :** https://github.com/ollama/ollama/blob/main/docs/api.md
- **Mod√®les disponibles :** https://ollama.com/library

---

**Derni√®re mise √† jour :** 6 janvier 2026
